## Data Quality and Risk Insights for Operational Safety and Visibility

### End-to-End Data Analysis & Deduplication Project
You can view the interactive version of this project here:
[View Project in Browser](https://marya-d2307.github.io/MaryaD_Portfolio.github.io/professional_projects/deduplication_project/Deduplication_Project.html)


This project simulates real-world challenges faced by organizations operating large vehicle fleets, with a focus on data quality, deduplication, risk event detection, and operational visibility. It demonstrates a structured, business-oriented approach to identifying and mitigating risks across driver behavior, freight tracking, vehicle faults, and maintenance data.

### Project Structure

The project is divided into modular, realistic components reflecting different facets of operational data:

| Section                          | Description                                                                                           |
|-----------------------------------|-------------------------------------------------------------------------------------------------------|
| 1. Risk Events Analysis           | Detects and visualizes risky driving events, cleans duplicates/missing data, and generates simple risk scores for drivers. |
| 2. Freight Visibility             | Simulates asset location tracking, identifies data quality gaps, and visualizes geographic distribution of tracked assets. |
| 3. Risky Driver & Event Mapping   | Aggregates driver risk profiles and visualizes event locations using interactive maps.                |
| 4. Vehicle Faults Analysis        | Explores common vehicle fault patterns, estimates operational downtime, and applies a basic predictive model for duplicate detection. |
| 5. Maintenance Data Quality       | Identifies inconsistencies (e.g., negative odometer readings) and duplicates in vehicle maintenance records. |
| 6. Final Recommendations          | Provides actionable suggestions for improving data pipelines, risk detection, and deduplication processes. |

### Key Features & Techniques

- Data Cleaning: Systematic handling of missing values, duplicates, and inconsistent records  
- Exploratory Data Analysis (EDA): Summary statistics, visual insights, and distribution checks  
- Geospatial Visualizations: Interactive maps of risk events and freight locations  
- Risk Scoring: Simple but interpretable risk aggregation for drivers  
- Predictive Deduplication Prototype: Random Forest model to simulate duplicate detection  
- Business-Oriented Recommendations: Practical next steps to enhance data quality and operational safety  

### Tools & Libraries

- Python (Pandas, NumPy, Matplotlib, Seaborn, Folium, Scikit-learn)  
- Jupyter Notebooks for iterative analysis  
- Simulated datasets for realistic scenario building  

### Project Motivation

This project was designed as a self-initiated learning exercise to develop practical skills aligned with data quality, deduplication, and operational risk management roles. It reflects an understanding of how poor data quality, undetected duplicates, and inconsistent information can impact business outcomes such as safety, asset visibility, and operational efficiency.

### Future Enhancements

- Integrating SQL-based quality checks  
- Scaling deduplication models with more sophisticated ML techniques  
- Simulating real-time data pipelines and monitoring  
- Exploring production-ready deployment for data quality solutions  

### Notes

This project was developed with guidance from AI tools (e.g., ChatGPT) for structure and coding assistance, but all logic, interpretation, and business framing were designed and understood by the author. Some intentional AI-generated inconsistencies were identified and corrected during the process to ensure quality.
